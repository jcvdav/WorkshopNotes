---
title: "Machine Learning with R"
author: "Juan Carlos Villase√±or-Derbez"
date: "14 de diciembre de 2017"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r, echo = F, warnings = F, message = F}
suppressPackageStartupMessages({
  library(tidyverse)
})
```


# Supervised Learning: Classification

##  k-Nearest Neighbors (kNN)

[Slides](Slides/Course1/chapter1.pdf)

### Recognizing a road sign with kNN

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

![](Data/1Classification/knn_stop_99.gif)

Can you apply a kNN classifier to help the car recognize this sign?

**Instructions**

```{r, echo = FALSE}

all_signs <- read.csv("Data/1Classification/knn_traffic_signs.csv")
  
signs <- all_signs %>% 
  filter(sample == "train") %>% 
  select(-c(1,2))

next_sign <- all_signs[206, ] %>% 
  select(-c(1,2, 3))
```


The dataset `signs` is loaded in your workspace along with the dataframe `next_sign`, which holds the observation you want to classify.

- Load the `class` package.
- Create a vector of sign labels to use with kNN by extracting the column `sign_type` from `signs`.
- Identify the `next_sign` using the `knn()` function.
- Set the `train` argument equal to the `signs` data frame without the first column.
- Set the `test` argument equal to the data frame `next_sign`.
- Use the vector of labels you created as the `cl` argument.

```{r}
# Load the 'class' package
library(class)

# Create a vector of labels
sign_types <- signs$sign_type

# Classify the next sign observed
knn(train = signs[,-1], test = next_sign, cl = sign_types)
```

### Exploring the traffic sign dataset

To better understand how the `knn()` function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![](Data/1Classification/knn_sign_data.png)

The result is a dataset that records the `sign_type` as well as 16 x 3 = 48 color properties of each sign.

**Instructions**

- Use the `str()` function to examine the `signs` dataset.
- Use `table()` to count the number of observations of each sign type by passing it the column containing the labels.
- Run the provided `aggregate()` command to see whether the average red level might vary by sign type.

```{r}
# Examine the structure of the signs dataset
# str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

### Classifying a collection of road signs

Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 59 additional road signs divided into three types:

![](Data/1Classification/knn_stop_28.gif)![](Data/1Classification/knn_speed_55.gif)![](Data/1Classification/knn_peds_47.gif)

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

**Intructions**

```{r, echo = FALSE}
test_signs <- all_signs %>% 
  filter(!sample == "train") %>% 
  select(-c(1,2))
```


The class package and the dataset signs are already loaded in your workspace. So is the dataframe test_signs, which holds a set of observations you'll test your model on.

- Classify the `test_signs` data using `knn()`.
- Set `train` equal to the observations in `signs` without labels.
- Use `test_signs` for the `test` argument, again without labels.
- For the `cl` argument, use the vector of labels provided for you.
- Use `table()` to explore the classifier's performance at identifying the three sign types.
- Create the vector `signs_actual` by extracting the labels from `test_signs`.
- Pass the vector of predictions and the vector of actual signs to `table()` to cross tabulate them.
- Compute the overall accuracy of the kNN learner using the `mean()` function

```{r}
# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the actual versus predicted values
signs_actual <- test_signs$sign_type
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

### Testing other 'k' values

By default, the `knn()` function in the class package uses only the single nearest neighbor.

Setting a `k` parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare `k` values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

**Instructions**

```{r, echo = FALSE}
signs_test <- all_signs %>% 
  filter(!sample == "train") %>% 
  select(-c(1,2))
```

The class package is already loaded in your workspace along with the datasets signs and signs_test. The object signs_actual holds the true values of the signs.

- Compute the accuracy of the default `k = 1` model using the given code.
- Modify the `knn()` function call by setting `k = 7`.
- Revise the code once more by setting `k = 15` and compare the three accuracy values.

```{r}
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, k = 1)
mean(k_1 == signs_test$sign_type)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, k = 7)
mean(k_7 == signs_test$sign_type)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, k = 15)
mean(k_15 == signs_test$sign_type)
```

### Seeing how the neighbors voted

When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the `knn()` function.

**Instructions**

The class package has already been loaded in your workspace along with the dataset signs.

- Build a kNN model with the `prob = TRUE` parameter to compute the vote proportions. Set `k = 7`.
- Use the `attr()` function to obtain the vote proportions for the predicted class. These are stored in the attribute `"prob"`.
- Examine the first several vote outcomes and percentages using the `head()` function to see how the confidence varies from sign to sign.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, prob = T, k = 7)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

## Naive Bayes

### Computing probabilities

The `where9am` data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his `location` at 9am each day as well as whether the daytype was a `weekend` or `weekday`.

Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

$$P(A\mid B)=\frac{P(A\land B)}{P(B)}$$

Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

**Instructions**

```{r}
locations <- read.csv("Data/1Classification/locations.csv")

where9am <- locations %>% 
  filter(hour == 9, hourtype == "morning") %>% 
  select(daytype, location)
```


- Find $P(office)$ using `nrow()` and `subset()` to count rows in the dataset and save the result as `p_A`.
- Find $P(weekday)$, using `nrow()` and `subset()` again, and save the result as `p_B`.
- Use `nrow()` and `subset()` a final time to find $P(office \land weekday)$. Save the result as `p_AB`.
- Compute $P(office \mid weekday)$ and save the result as `p_A_given_B`.
- Print the value of `p_A_given_B`.

```{r}
# Compute P(A) 
p_A <- nrow(subset(where9am, location == "office"))/nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday"))/nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office", daytype == "weekday"))/nrow(where9am)

# Compute P(A | B)
p_A_given_B <- p_AB/p_B
p_A_given_B
```

### A simple Naive Bayes location model

The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.

To see this finding in action, use the `where9am` data frame to build a Naive Bayes model on the same data.

You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

**Instructions**

```{r, echo = FALSE}
thursday9am <- data.frame(daytype = "weekday")
saturday9am <- data.frame(daytype = "weekend")
```


The dataframe where9am is available in your workspace. This dataset contains information about Brett's location at 9am on different days.

- Load the `naivebayes` package.
- Use `naive_bayes()` with a formula like `y ~ x` to build a model of `location` as a function of `daytype`.
- Forecast the Thursday 9am location using `predict()` with the `thursday9am` object as the newdata argument.
- Do the same for predicting the `saturday9am` location.

```{r, warning = FALSE, message = FALSE}
# Load the naivebayes package
library(naivebayes)

# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
predict(locmodel, thursday9am)

# Predict Saturdays's 9am location
predict(locmodel, saturday9am)
```

**https://campus.datacamp.com/courses/supervised-learning-in-r-classification/chapter-2-naive-bayes?ex=5**

## Logistic Regression

## Classification Trees


# Supervised Learning: Regression



# Unsupervised Learning



# Machine Learning Toolbox








